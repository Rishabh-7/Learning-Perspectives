Ensemble Learning

Procedures employed to train multiple learning machines
Then combine their output making a committee of decision makers.

Should have better overall accuracy on average than any individual commitee member.

Members might be predicting.
Real valued numbers
Class labels
Posteroir probablilities
Ranking etc.

Main aim is to manage strengths and weaknesses leading to the best possible decision .

Things to keep in mind while slelecting model
-use cross validation
-choose direuse models
-outcomes of each model are usually averaged out
-final goal to reduce risk
-stability of your data sample

if data is plenty,split it into several parts  - train,validation test
Train each classifier on a seperate subset.
Not big in size the use different bootstrap samples of data

Devide and Conquer

devide the dataspace into smaller and easier to learn partitions.
Here each classifier learns only one of the simpler partitions.


Data fusion


suitable combination of data from different sources -- lead to improved accuracy.

Confidence estimation

If the majority of classifiers agree with their decision then high confidence 
If half classifiers make one decision and other half make different decision then low confidence

high confidence does not mean decision is good , but usually properly trained ensemble decision then usually good.

Ensemble learning algorithms

1. Bagging/bootstrap

diversity is obtained by using bootstraped replicas.

2 Boosting

similar to bagging but resampling is strategically geared to provide the most informative training data.
designed for binary class problems.

3.AdaBoost(Adaptive boosting)
4.Random forests
another version of bagging 
trained with bagging mechanism
naturally handle both regression and multiclass classification.
fast to train.
depends only on one or two tuning parameters
Ways of Combining Ensemble Members
Majority voting
weighted Majority voting.
Borda Count

